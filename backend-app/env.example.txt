# LLM Configuration
LLM_PROVIDERS__OPENAI__API_KEY="your-api-key" # Required: For OpenAI
LLM_PROVIDERS__GEMINI__API_KEY="your-api-key" # Optional: For Gemini
LLM_PROVIDERS__ANTHROPIC__API_KEY="your-api-key" # Optional: For Anthropic

LLM_PROVIDERS__OPENAI__MODEL_NAME="gpt-3.5-turbo" # Default primary model to use
LLM_PROVIDERS__ANTHROPIC__MODEL_NAME="claude-3" # Optional: Fallback LLM model
LLM_PROVIDERS__GEMINI__MODEL_NAME="gemini-1.5" # Optional: Fallback LLM model

LLM_PROVIDERS__OPENAI__ENDPOINT="https://api.openai.com/v1" # Example: For OpenAI
LLM_PROVIDERS__GEMINI__ENDPOINT="https://api.gemini.com/v1"
LLM_PROVIDERS__ANTHROPIC__ENDPOINT="https://api.anthropic.com/v1" # Example: For Anthropic

# AZURE_OPENAI_ENDPOINT="https://..." (If using Azure)
# AZURE_OPENAI_API_KEY="..."
PRIMARY_LLM_PROVIDER="openai" # e.g., "openai", "anthropic"
FALLBACK_LLM_PROVIDER="anthropic" # Optional: e.g., "openai"

LLM_REQUEST_TIMEOUT=60
LLM_MAX_RETRIES=3

# API Configuration (If applicable)
API_HOST="0.0.0.0"
API_PORT="8000"

# Memory Configuration (Example for Redis)
MEMORY_TYPE="redis" # or "file"
REDIS_HOST="localhost"
REDIS_PORT="6379"
REDIS_PASSWORD="" # Optional
REDIS_DB="0"

# Service Configuration
DEFAULT_REQUEST_TIMEOUT="60" # Default timeout in seconds for external calls
MAX_RETRY_ATTEMPTS="3"      # Default max retry attempts

# Agent Configuration
AGENT_GRAPH_CONFIG_DIR="src/config/agent_graphs" # Directory for dynamic graph JSON files
PROMPT_TEMPLATE_DIR="src/config/prompts" # Directory for prompt templates
# DEFAULT_AGENT_GRAPH="default_workflow.json" # Default graph config to load if not specified

# Observability Configuration
OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4317" # Example OTLP endpoint
LOG_LEVEL="INFO" # e.g., DEBUG, INFO, WARNING, ERROR

# LangSmith (Optional, for LangChain Tracing)
LANGCHAIN_TRACING_V2="true"
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_API_KEY="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"
LANGCHAIN_PROJECT="pr-enchanted-wasabi-35" # Optional: LangSmith project name
# LANGSMITH_OTEL_ENABLED="true" # Enable if sending OTel data via LangSmith SDK