# .env.example - Copy to .env and fill in your values

# LLM Configuration
LLM_PROVIDERS__OPENAI__API_KEY="your_openai_api_key" # Required: For OpenAI
LLM_PROVIDERS__GEMINI__API_KEY="your_gemini_api_key" # Optional: For Gemini
LLM_PROVIDERS__ANTHROPIC__API_KEY="your_anthropic_api_key" # Optional: For Anthropic

LLM_PROVIDERS__OPENAI__MODEL_NAME="gpt-3.5-turbo" # Default primary model to use
LLM_PROVIDERS__ANTHROPIC__MODEL_NAME="claude-2" # Optional: Fallback LLM model
LLM_PROVIDERS__GEMINI__MODEL_NAME="gemini-1.5" # Optional: Fallback LLM model

LLM_PROVIDERS__OPENAI__ENDPOINT="https://api.openai.com/v1" # Example: For OpenAI
LLM_PROVIDERS__GEMINI__ENDPOINT="https://api.gemini.com/v1"
LLM_PROVIDERS__ANTHROPIC__ENDPOINT="https://api.anthropic.com/v1" # Example: For Anthropic

# AZURE_OPENAI_ENDPOINT="https://..." (If using Azure)
# AZURE_OPENAI_API_KEY="..."
PRIMARY_LLM_PROVIDER="openai" # e.g., "openai", "anthropic"
FALLBACK_LLM_PROVIDER="anthropic" # Optional: e.g., "openai"

LLM_REQUEST_TIMEOUT=60
LLM_MAX_RETRIES=3

# API Configuration (If applicable)
API_HOST="0.0.0.0"
API_PORT="8000"

# Memory Configuration (Example for Redis)
MEMORY_TYPE="redis" # or "file"
REDIS_HOST="localhost"
REDIS_PORT="6379"
REDIS_PASSWORD="" # Optional
REDIS_DB="0"

# Service Configuration
DEFAULT_REQUEST_TIMEOUT="60" # Default timeout in seconds for external calls
MAX_RETRY_ATTEMPTS="3"      # Default max retry attempts

# Agent Configuration
AGENT_GRAPH_CONFIG_DIR="config/agent_graphs" # Directory for dynamic graph JSON files
DEFAULT_AGENT_GRAPH="default_workflow.json" # Default graph config to load if not specified

# Observability Configuration
OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4317" # Example OTLP endpoint
LOG_LEVEL="INFO" # e.g., DEBUG, INFO, WARNING, ERROR

# LangSmith (Optional, for LangChain Tracing)
LANGCHAIN_TRACING_V2="true"
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_API_KEY="ls__..."
LANGCHAIN_PROJECT="MyMultiAgentProject" # Optional: LangSmith project name
# LANGSMITH_OTEL_ENABLED="true" # Enable if sending OTel data via LangSmith SDK