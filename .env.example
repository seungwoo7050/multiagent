# .env.example
# Example environment variables for the application.
# Copy this file to .env and fill in your actual values for deployment.

# --- Application Settings ---
# Application environment (development | production)
ENVIRONMENT=development
# Enable debug mode (True | False)
DEBUG=False
# Unique identifier for this node/instance (optional, useful in clustered deployments)
# NODE_ID=node-1

# --- Logging Settings ---
# Log level (DEBUG | INFO | WARNING | ERROR | CRITICAL)
LOG_LEVEL=INFO
# Log format (json | text)
LOG_FORMAT=json
# Enable logging to a file (True | False)
LOG_TO_FILE=False
# Log file path (used if LOG_TO_FILE=True)
# LOG_FILE_PATH=./logs/app.log

# --- Worker & Task Settings ---
# Number of worker processes (defaults to CPU core count)
# WORKER_COUNT=4
# Maximum concurrent tasks the system can handle overall
# MAX_CONCURRENT_TASKS=100
# Default task queue implementation settings (if applicable, e.g., for Redis Streams)
# TASK_QUEUE_STREAM_NAME=task_stream
# TASK_QUEUE_GROUP_NAME=orchestration_group
# TASK_QUEUE_CLAIM_INTERVAL_MS=60000
# Task status TTL in memory (seconds)
# TASK_STATUS_TTL=86400

# --- Connection Settings ---
# Default request timeout (seconds)
REQUEST_TIMEOUT=30.0
# Enable performance tracking via metrics (True | False)
ENABLE_PERFORMANCE_TRACKING=True

# --- Redis Settings ---
# Redis connection URL (e.g., redis://user:password@host:port/db)
REDIS_URL=redis://localhost:6379/0
# Redis password (leave empty if none)
# REDIS_PASSWORD=your_redis_password
# Redis connection pool size
# REDIS_CONNECTION_POOL_SIZE=20

# --- Memory Settings ---
# Default TTL for general memory items (seconds)
MEMORY_TTL=86400
# Default TTL for L1/L2 caches (seconds)
CACHE_TTL=3600
# Cache lookup timeout (seconds, for with_cache decorator)
# CACHE_LOOKUP_TIMEOUT=1.0
# Cache save timeout (seconds, for with_cache decorator)
# CACHE_SAVE_TIMEOUT=2.0

# --- LLM Settings ---
# Primary LLM model to use by default
PRIMARY_LLM=gpt-4o
# Fallback LLM model if the primary fails
FALLBACK_LLM=gpt-3.5-turbo
# Max retry attempts for LLM API calls
LLM_RETRY_MAX_ATTEMPTS=3
# Base delay factor for LLM retry backoff
# LLM_RETRY_BACKOFF_FACTOR=0.5
# Enable jitter in LLM retry backoff (True | False)
# LLM_RETRY_JITTER=True

# Enabled models (comma-separated string or JSON list string)
# Example: ENABLED_MODELS_SET="gpt-4o,gpt-3.5-turbo,claude-3-sonnet"
ENABLED_MODELS_SET='["gpt-4o", "gpt-3.5-turbo", "claude-3-sonnet", "claude-3-haiku"]'

# --- LLM Provider API Keys ---
# API keys for the respective LLM providers.
# These are loaded into settings.LLM_PROVIDERS_CONFIG if not set there directly.
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
# Add other provider keys if needed, e.g., # MISTRAL_API_KEY=...

# --- Agent Settings ---
# Path to the agent configuration file (relative to project root or absolute)
AGENT_CONFIG_FILE_PATH=configs/agent_configs.json
# Default name for the planner agent (must match a name in AGENT_CONFIG_FILE_PATH)
PLANNER_AGENT_NAME=default_planner
# Default name for the executor agent (must match a name in AGENT_CONFIG_FILE_PATH)
EXECUTOR_AGENT_NAME=default_executor

# --- API Settings ---
# Host for the API server
API_HOST=0.0.0.0
# Port for the API server
API_PORT=8000
# API endpoint prefix (e.g., /api/v1)
# API_PREFIX=/api/v1
# Allowed origins for CORS (comma-separated string or JSON list string, '*' for all)
CORS_ORIGINS='["*"]' # Example: Allow all origins
# Require authentication for API endpoints (True | False) - Currently disabled based on requirements
AUTH_REQUIRED=False
# Authentication token expiry time (seconds)
# AUTH_TOKEN_EXPIRY=86400
# Secret key for signing authentication tokens (generate a strong random key)
# SECRET_KEY=your_strong_random_secret_key

# --- Search API Settings ---
# Google Custom Search API Key
# Google Search_API_KEY=your_Google Search_api_key
# Google Custom Search Engine ID
# Google Search_ENGINE_ID=your_Google Search_engine_id
# DuckDuckGo Proxy API URL (can be self-hosted or public)
DUCKDUCKGO_PROXY_API_URL=https://api.duckduckgo.com/
# TTL for search results cache (seconds)
SEARCH_CACHE_TTL=3600

# --- Metrics Settings ---
# Enable Prometheus metrics endpoint (True | False)
METRICS_ENABLED=True
# Port for the metrics server
METRICS_PORT=9090

# --- Vector DB Settings ---
# Type of vector database (chroma | qdrant | faiss | none)
VECTOR_DB_TYPE=none
# Connection URL or path for the vector database (used if VECTOR_DB_TYPE is not 'none')
# VECTOR_DB_URL=http://localhost:6333 # Example for Qdrant
# VECTOR_DB_URL=./vector_data/chroma # Example for Chroma persistent path
# VECTOR_DB_URL= # Leave empty for in-memory FAISS/Chroma

# --- Worker Pool Scaling (Optional - for worker_manager.py if used) ---
# Minimum number of workers
# WORKER_MIN_COUNT=1
# Maximum number of workers
# WORKER_MAX_COUNT=8
# Queue size factor to trigger scale-up (Queue Depth > Active Workers * Factor)
# WORKER_SCALE_UP_QUEUE_FACTOR=5.0
# Idle utilization threshold to trigger scale-down (Active Workers / Max Workers < Threshold)
# WORKER_SCALE_DOWN_IDLE_THRESHOLD=0.2
# Minimum interval between scaling actions (seconds)
# WORKER_MIN_SCALE_INTERVAL_S=60